{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854c9a67-475c-4403-8d09-1d8e39ca7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1de1a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import ujson, os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d92475-7f02-40ef-8c6f-4799ea509c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"concurrentqa/\" # FILL IN PATH TO REPOSITORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a8bcb-4d2c-4935-ad4d-b24669b49d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db0daf39",
   "metadata": {},
   "source": [
    "# Load original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1053be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full set of documents in 110.8920521736145\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "passages_path = f'{prefix}/datasets/hotpotqa/hotpot_index/wiki_id2doc.json'\n",
    "with open(passages_path) as f:\n",
    "    wiki_id2doc = json.load(f)\n",
    "    passages = []\n",
    "    for k, v in wiki_id2doc.items():\n",
    "        v['id'] = k\n",
    "        passages.append(v)\n",
    "        \n",
    "print(f\"Loaded full set of documents in {time.time() - st}\")\n",
    "st = time.time()\n",
    "\n",
    "st = time.time()\n",
    "df = pd.DataFrame(passages)\n",
    "print(f\"Loaded full set of documents in {time.time() - st}\")\n",
    "st = time.time()\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2faf41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17394/5233329 [00:00<00:30, 173827.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One Night Stand is a 1984 film directed by John Duigan.']\n",
      "\n",
      "['Welty McCullogh (October 10, 1847 – August 31, 1889) was a Republican member of the U.S', 'House of Representatives from Pennsylvania.']\n",
      "\n",
      "['The Neuropterida are a clade of holometabolous insects with over 5,700 described species, containing the orders Neuroptera (lacewings, antlions), Megaloptera (dobsonflies, alderflies), and Raphidioptera (snakeflies).']\n",
      "\n",
      "['Bafia (beukpak) people inhabit the Mbam region in the centre province of Cameroon', 'Their origins are said to share many similarities with those of the Bamun and Tikar people', 'A division during migratory movements caused the two sets of groups to settle in different areas', 'Later, the islamisation of most of the Bamun territory further separated them', 'A yearly festival held in Fumban (Bamun territory) is considered by many to symbolize the recognition of their common heritage.']\n",
      "\n",
      "['The Viti Levu giant pigeon (\"Natunaornis gigoura\") is an extinct flightless pigeon of Viti Levu, the largest island in Fiji', 'It was only slightly smaller than the dodo (\"Raphus cucullatus\") and Rodrigues solitaire (\"Pezophaps solitaria\").']\n",
      "\n",
      "['Glusburn is a village, electoral ward and civil parish situated in Craven in North Yorkshire, England', 'Historically part of the West Riding of Yorkshire, the village is situated on the edge of the Yorkshire Dales, sits on the A6068 Kildwick to Hapton road, and is conjoined to the village of Sutton-in-Craven at the south.']\n",
      "\n",
      "['Labor Notes is a non-profit organization and network for rank-and-file union members and grassroots labor activists', 'Though officially titled the Labor Education and Research Project, the project is best known by the title of its monthly magazine', 'The magazine reports news and analysis about labor activity or problems facing the labor movement', 'In its pages it advocates for a revitalization of the labor movement through Social Movement Unionism and union democracy', 'Labor Notes is based out of Detroit, Michigan with an East Coast office located in Brooklyn, New York.']\n",
      "\n",
      "['Zombie is a studio album by Nigerian Afrobeat musician Fela Kuti', 'It was released in Nigeria by Coconut Records in 1976, and in the United Kingdom by Creole Records in 1977.']\n",
      "\n",
      "['\"The Nutcracker and the Mouse King\" (German: \"Nussknacker und Mausekönig\" ) is a story written in 1816 by German author E', 'T', 'A', \"Hoffmann, in which young Marie Stahlbaum's favorite Christmas toy, the Nutcracker, comes alive and, after defeating the evil Mouse King in battle, whisks her away to a magical kingdom populated by dolls\", 'In 1892, the Russian composer Pyotr Ilyich Tchaikovsky and choreographers Marius Petipa and Lev Ivanov turned Alexandre Dumas père\\'s adaptation of the story into the ballet \"The Nutcracker\", which became one of Tchaikovsky\\'s most famous compositions, and perhaps the most popular ballet in the world.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5233329/5233329 [01:44<00:00, 49845.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5233329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "alldomaintitles = []\n",
    "title2sent_map = {}\n",
    "count = 0\n",
    "for k, v in tqdm(wiki_id2doc.items()):\n",
    "    title = v['title']\n",
    "    sents = v['text']\n",
    "    sents = sents.split(\". \")\n",
    "    \n",
    "    alldomaintitles.append(title)\n",
    "    title2sent_map[title] = sents\n",
    "        \n",
    "print(len(title2sent_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{prefix}/datasets/hotpotqa/hotpot/hotpot_qas_val.json') as f:\n",
    "    qa_entries = []\n",
    "    for line in f:\n",
    "        entry = ast.literal_eval(line)\n",
    "        qa_entries.append(entry)\n",
    "print(f\"QAs set has {len(qa_entries)} data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe74f-bd2e-46c3-b188-3c3b5dcb6e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bff65c-09b6-419b-b6e3-0bd7822fd416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "315cbd2c",
   "metadata": {},
   "source": [
    "### The following generates new private/public data splits and prepares everything for running retrieval on HotpotQA Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9a033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(item, domain=-1):\n",
    "    item['domain'] = domain\n",
    "    return item\n",
    "\n",
    "def get_domain_splits(private_prop, alldomaintitles):\n",
    "    title2domain = {}\n",
    "    domain1titles = []\n",
    "    domain2titles = []\n",
    "    random.seed(0)\n",
    "    \n",
    "    # splits by title randomly \n",
    "    for title in tqdm(alldomaintitles):\n",
    "        randnum = random.random()\n",
    "        if randnum > private_prop:\n",
    "            title2domain[title] = 0\n",
    "            domain1titles.append(title)\n",
    "        else:\n",
    "            title2domain[title] = 1\n",
    "            domain2titles.append(title)\n",
    "            \n",
    "    return title2domain, domain1titles, domain2titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c1e027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7405it [00:00, 156836.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 1837 local questions\n",
      "There are: 1823 global questions\n",
      "For 0 questions, could not find documents in corpus.\n",
      "Fore 0 questions, contains private and public entities.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we will do this for private splits of different sizes; private_prop 0.5 means equal 50-50 public-private splits\n",
    "for private_prop in [0.5]:\n",
    "    # path where you will save the generated data\n",
    "    path = f\"{prefix}/datasets/hotpotqa_pair/hotpot_privateprop_{private_prop}/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    title2domain, domain1titles, domain2titles = get_domain_splits(private_prop, alldomaintitles)\n",
    "            \n",
    "    # Save the passages\n",
    "    print(f\"Num domain 1 titles: {len(domain1titles)}\")\n",
    "    print(f\"Num domain 2 titles: {len(domain2titles)}\\n\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Making dir at: {path}\")\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    df['domain1'] = df['title'].apply(lambda x: title2domain[x]==0)\n",
    "    df['domain2'] = df['title'].apply(lambda x: title2domain[x]==1)\n",
    "    \n",
    "    sub_df = df[df['domain1'] == True]\n",
    "    dic = sub_df.to_dict('index')\n",
    "    with open(f'{path}/domain0psgs.json', \"w\") as f:\n",
    "        json.dump(dic, f)\n",
    "    print(\"Saved domain 1 passages.\")\n",
    "\n",
    "    sub_df = df[df['domain2'] == True]\n",
    "    dic = sub_df.to_dict('index')\n",
    "    with open(f'{path}/domain1psgs.json', \"w\") as f:\n",
    "        json.dump(dic, f)\n",
    "    print(\"Saved domain 2 passages.\")\n",
    "        \n",
    "    # determine private and public entities of those appearing in the queries \n",
    "    entity_df = pd.DataFrame(entitytitles.items(), columns=['entitytitle', 'domain'])\n",
    "    entity_df['domain'] = entity_df['entitytitle'].apply(lambda x: title2domain[x]==0)\n",
    "    entitytitle2domain_cache = {}\n",
    "    for ind, row in entity_df.iterrows():\n",
    "        entitytitle2domain_cache[row['entitytitle']] = row['domain']\n",
    "    \n",
    "    # Save the questions\n",
    "    localquestions = []\n",
    "    globalquestions = []\n",
    "    not_in_corpus = 0\n",
    "    for idx, item in tqdm(enumerate(qa_entries)):\n",
    "        sps = item['sp']\n",
    "        \n",
    "        domains  = [entitytitle2domain_cache[sp['title']] for sp in sps]\n",
    "        domain1_exists = any(d == True  for d in domains)\n",
    "        domain2_exists = any(d == False for d in domains)\n",
    "        neither_exists = not domain1_exists and not domain2_exists\n",
    "        \n",
    "        if domain1_exists and not domain2_exists:\n",
    "            localquestions.append(process_item(item, domain=domains))\n",
    "        if not domain1_exists and domain2_exists:\n",
    "            globalquestions.append(process_item(item, domain=domains))\n",
    "        if neither_exists:\n",
    "            not_in_corpus += 1\n",
    "\n",
    "    print(f\"There are: {len(localquestions)} local questions\")\n",
    "    print(f\"There are: {len(globalquestions)} global questions\")\n",
    "    print(f\"For {not_in_corpus} questions, could not find documents in corpus.\")\n",
    "    \n",
    "    if not os.path.exists(f'{path}/domain_0/'):\n",
    "        os.makedirs(f'{path}/domain_0/')\n",
    "    if not os.path.exists(f'{path}/domain_1/'):\n",
    "        os.makedirs(f'{path}/domain_1/')\n",
    "    \n",
    "    with open(f'{path}/domain_0/hotpot_qas_val_domain0.json', 'w') as f:\n",
    "        for question in localquestions:\n",
    "            json.dump(question, f)\n",
    "            f.write(\"\\n\")\n",
    "    with open(f'{path}/domain_1/hotpot_qas_val_domain1.json', 'w') as f:\n",
    "        for question in globalquestions:\n",
    "            json.dump(question, f)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    allquestions = localquestions.copy()\n",
    "    allquestions.extend(globalquestions.copy())\n",
    "    with open(f'{path}/hotpot_qas_val_all.json', 'w') as f:\n",
    "        for question in allquestions:\n",
    "            json.dump(question, f)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    print(f\"Saved data for private proportion {private_prop}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ac87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
