{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059\n",
      "12273\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "fever_path = \"/private/home/xwhan/data/fever/retrieval/\"\n",
    "\n",
    "dev = [json.loads(l) for l in open(fever_path + \"dev.txt\").readlines()]\n",
    "multi_dev = []\n",
    "single_dev = []\n",
    "all_evidence_lens = [] # for multi evidence\n",
    "random.shuffle(dev)\n",
    "all_claim_lens = []\n",
    "for item in dev:\n",
    "    evidence_lens = []\n",
    "    all_claim_lens.append(len(item[\"claim\"].split()))\n",
    "    \n",
    "    for chain in item[\"evidence\"]:\n",
    "        if len(chain) > 1:\n",
    "#             evidence_lens.append(len(chain))\n",
    "            chain_titles = set([p[\"title\"] for p in chain])\n",
    "            evidence_lens.append(len(chain_titles))  \n",
    "#             print(item[\"claim\"])\n",
    "#             print(chain)\n",
    "#             assert False\n",
    "        else:\n",
    "            evidence_lens.append(1)\n",
    "    multi_count = np.sum([int(c > 1) for c in evidence_lens])\n",
    "    \n",
    "    if multi_count == len(evidence_lens):\n",
    "        multi_dev.append(item)\n",
    "        all_evidence_lens += evidence_lens\n",
    "    else:\n",
    "        single_dev.append(item)\n",
    "        \n",
    "print(len(multi_dev))\n",
    "print(len(single_dev))\n",
    "with open(\"/private/home/xwhan/data/fever/retrieval/dev_multi_evidence_compact.txt\", \"w\") as g:\n",
    "    for l in multi_dev:\n",
    "        g.write(json.dumps(l) + \"\\n\")\n",
    "# with open(\"/private/home/xwhan/data/fever/retrieval/dev_single_evidence.txt\", \"w\") as g:\n",
    "#     for l in single_dev:\n",
    "#         g.write(json.dumps(l) + \"\\n\")\n",
    "# with open(\"/private/home/xwhan/data/fever/retrieval/dev_all.txt\", \"w\") as g:\n",
    "#     for l in single_dev + multi_dev:\n",
    "#         g.write(json.dumps(l) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741\n",
      "2.0\n",
      "0.5835726593911545 0.5835726593911545 0.5835726593911545\n"
     ]
    }
   ],
   "source": [
    "# baseline retrieval for single/multihop subsets\n",
    "\n",
    "import unicodedata\n",
    "def normalize(text):\n",
    "    \"\"\"Resolve different type of unicode encodings.\"\"\"\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "el_results = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/dev.ensembles.s10.jsonl\").readlines()]\n",
    "id2el_docs = {_[\"id\"]:_[\"predicted_pages\"] for _ in el_results}\n",
    "\n",
    "dense_multi_results = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/dense_fever_b1_20_k20.json\").readlines()] \n",
    "\n",
    "single_gold = {_[\"id\"]:_ for _ in single_dev}\n",
    "multi_gold = {_[\"id\"]:_ for _ in multi_dev}\n",
    "all_gold = {_[\"id\"]:_ for _ in multi_dev + single_dev}\n",
    "\n",
    "subset = multi_gold\n",
    "precs, recalls = [], []\n",
    "doc_count = []\n",
    "dense_docs = []\n",
    "for item in dense_multi_results:\n",
    "    if item[\"id\"] in subset:\n",
    "#         pred = set(item[\"predicted_pages\"])\n",
    "        retrieved_chains = item[\"candidate_chains\"] \n",
    "        pred = []\n",
    "        for chain in retrieved_chains:\n",
    "            for p in chain:\n",
    "                if normalize(p[0]) not in pred:\n",
    "                    pred.append(normalize(p[0]))\n",
    "        pred = pred[:2]\n",
    "#         pred = [_[\"title\"] for _ in item[\"topk\"][:1]]\n",
    "        \n",
    "        pred = set(pred)\n",
    "#         el_pred = id2el_docs[item[\"id\"]]\n",
    "#         el_count = 0\n",
    "#         for title in el_pred:\n",
    "#             if title not in pred:\n",
    "#                 pred.add(title)\n",
    "#                 el_count +=1\n",
    "#                 if el_count == 2:\n",
    "#                     break\n",
    "        pred = list(pred)\n",
    "    \n",
    "        dense_docs.append({\n",
    "            \"claim\": item[\"claim\"],\n",
    "            \"id\": item[\"id\"],\n",
    "            \"predicted_pages\": list(pred)\n",
    "        })\n",
    "        \n",
    "        doc_count.append(len(pred))\n",
    "    \n",
    "        gold_docs = set()\n",
    "        recall = 0\n",
    "        for chain in subset[item[\"id\"]][\"evidence\"]:\n",
    "            chain_titles = set([normalize(p[\"title\"]) for p in chain])\n",
    "            for t in chain_titles: gold_docs.add(t)\n",
    "            chain_covered = [int(t in pred) for t in chain_titles]\n",
    "            if np.sum(chain_covered) == len(chain_titles):\n",
    "                recall = 1\n",
    "                break\n",
    "                \n",
    "        if len(gold_docs) > 0:\n",
    "            if len(pred) == 0:\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = np.mean([int(doc in gold_docs) for doc in pred])\n",
    "                \n",
    "        precs.append(prec)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "print(len(precs))\n",
    "print(np.mean(doc_count))\n",
    "pr, rec = np.mean(precs), np.mean(recalls)\n",
    "print(pr, rec, 2.0 * pr * rec / (pr + rec))\n",
    "\n",
    "# with open(\"/private/home/xwhan/data/fever/retrieval/dense_wiki_pages_top2.jsonl\", \"w\") as g:\n",
    "#     for _ in dense_docs:\n",
    "#         g.write(json.dumps(_) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741\n",
      "2.9959793222286044\n",
      "0.6764680633362424 1.0 0.8070157471297632\n"
     ]
    }
   ],
   "source": [
    "# inspect the FEVER results\n",
    "dev_all_results = [json.loads(l) for l in open(\"/private/home/xwhan/code/Transformer-XH/data/fever_dev_graph.json\").readlines()]\n",
    "subset = multi_gold\n",
    "precs, recalls = [], []\n",
    "doc_count = []\n",
    "\n",
    "for item in dev_all_results:\n",
    "    if item[\"qid\"] in subset:\n",
    "        \n",
    "        pred = [_[\"name\"] for _ in item[\"node\"]]\n",
    "        pred = set(pred)\n",
    "        pred = list(pred)\n",
    "        \n",
    "        doc_count.append(len(pred))\n",
    "    \n",
    "        gold_docs = set()\n",
    "        recall = 0\n",
    "        for chain in subset[item[\"qid\"]][\"evidence\"]:\n",
    "            chain_titles = set([normalize(p[\"title\"]) for p in chain])\n",
    "            for t in chain_titles: gold_docs.add(t)\n",
    "            chain_covered = [int(t in pred) for t in chain_titles]\n",
    "            if np.sum(chain_covered) == len(chain_titles):\n",
    "                recall = 1\n",
    "                break\n",
    "                \n",
    "        if len(gold_docs) > 0:\n",
    "            if len(pred) == 0:\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = np.mean([int(doc in gold_docs) for doc in pred])\n",
    "                \n",
    "        precs.append(prec)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "print(len(precs))\n",
    "print(np.mean(doc_count))\n",
    "pr, rec = np.mean(precs), np.mean(recalls)\n",
    "print(pr, rec, 2.0 * pr * rec / (pr + rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passage retrieval evaluation \n",
    "def fever_retrieval_eval(results, topk=5):\n",
    "    \n",
    "    precs, recalls = [], []\n",
    "    for item in results:\n",
    "        gold = item[\"correct_normalized\"]\n",
    "        pred = item[\"bm25_topk\"][:topk]\n",
    "        \n",
    "        if len(gold) > 0:\n",
    "            prec = np.mean([int(doc in gold) for doc in pred])\n",
    "        else:\n",
    "            prec = 1\n",
    "        recall = 0\n",
    "        for chain in item[\"evidence\"]:\n",
    "            chain_titles = set([normalize(p[\"title\"]) for p in chain])\n",
    "            chain_covered = [int(t in pred) for t in chain_titles]\n",
    "            if np.sum(chain_covered) == len(chain_titles):\n",
    "                recall = 1\n",
    "                break\n",
    "        precs.append(prec)\n",
    "        recalls.append(recall)\n",
    "    pr, rec = np.mean(precs), np.mean(recalls)\n",
    "    return pr, rec, 2.0 * pr * rec / (pr + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.12268811028144745, 0.5020103388856979, 0.19718537768662206)\n"
     ]
    }
   ],
   "source": [
    "tfidf_results = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/multi_dev_tfidf.txt\").readlines()]\n",
    "print(fever_retrieval_eval(tfidf_results, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741\n",
      "1741\n",
      "(0.6223243346735593, 0.46927053417576103, 0.5350675077296432)\n"
     ]
    }
   ],
   "source": [
    "# phrase_matching_results = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/all_dev.json\").readlines()]\n",
    "# phrase_matching_results = [json.loads(l.decode('utf-8').strip('\\r\\n')) for l in open(\"/private/home/xwhan/data/fever/retrieval/dev.ensembles.s10.jsonl\").readlines()]\n",
    "phrase_matching_results = [json.loads(l) for l in open(\"/private/home/xwhan/code/Transformer-XH/data/fever_dev_graph.json\").readlines()]\n",
    "# for _ in phrase_matching_results:\n",
    "#     _[\"id\"] = _[\"qid\"]\n",
    "\n",
    "phrase_matching_results = [_ for _ in phrase_matching_results if _[\"id\"] in multihop_ids]\n",
    "# phrase_matching_results = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/esim_mhop_dev.json\").readlines()]\n",
    "\n",
    "# json.dump(phrase_matching_results, open(\"/private/home/xwhan/data/fever/retrieval/dev_el_wiki_pages.jsonl\", \"w\"))\n",
    "\n",
    "print(len(phrase_matching_results))\n",
    "\n",
    "tfidf_results = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/multi_dev_tfidf.txt\").readlines()]\n",
    "print(len(tfidf_results))\n",
    "pred_lens = []\n",
    "def fever_retrieval_eval_phrase(tfidf_results, phrase_results, topk=5):\n",
    "    id2gold = {_[\"id\"]:_[\"correct_normalized\"] for _ in tfidf_results}\n",
    "    id2gold_evidence = {_[\"id\"]:_[\"evidence\"] for _ in tfidf_results}\n",
    "    precs, recalls = [], []\n",
    "    for item in phrase_results:\n",
    "        gold = id2gold[item[\"id\"]]\n",
    "#         print(gold)\n",
    "        retrieved_evidence = item[\"evidence\"] \n",
    "        pred = []\n",
    "        for e in retrieved_evidence:\n",
    "            pred.append(normalize(e[0]))\n",
    "        \n",
    "#         pred = item[\"predicted_pages\"] + item[\"wiki_results\"]\n",
    "#         pred = item[\"wiki_results\"]\n",
    "#         pred = item[\"predicted_pages\"]\n",
    "        \n",
    "#         pred = []\n",
    "#         for n in item[\"node\"]:\n",
    "#             pred.append(n[\"name\"])\n",
    "        \n",
    "        pred = list(set(pred))\n",
    "        pred_lens.append(len(pred))\n",
    "        \n",
    "        if len(gold) > 0:\n",
    "            if len(pred) == 0:\n",
    "                prec = 0\n",
    "            else:\n",
    "                prec = np.mean([int(doc in gold) for doc in pred])\n",
    "        else:\n",
    "            prec = 1\n",
    "        recall = 0\n",
    "        for chain in id2gold_evidence[item[\"id\"]]:\n",
    "            chain_titles = set([normalize(p[\"title\"]) for p in chain])\n",
    "            chain_covered = [int(t in pred) for t in chain_titles]\n",
    "            if np.sum(chain_covered) == len(chain_titles):\n",
    "                recall = 1\n",
    "                break\n",
    "        precs.append(prec)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    pr, rec = np.mean(precs), np.mean(recalls)\n",
    "    return pr, rec, 2.0 * pr * rec / (pr + rec)\n",
    "print(fever_retrieval_eval_phrase(tfidf_results, phrase_matching_results, topk=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1039632395175185"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12426242624262426"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_with_all = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/all_dev.json\").readlines()]\n",
    "title_count = []\n",
    "for item in test_with_all:\n",
    "    titles = set()\n",
    "    for e in item[\"evidence\"]:\n",
    "        titles.add(e[0])\n",
    "    title_count.append(len(titles))\n",
    "np.sum(np.array(title_count) > 7) / len(title_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11321132113211321"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_with_all = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/all_test.json\").readlines()]\n",
    "title_count = []\n",
    "for item in test_with_all:\n",
    "    titles = set()\n",
    "    for e in item[\"evidence\"]:\n",
    "        titles.add(e[0])\n",
    "    title_count.append(len(titles))\n",
    "np.sum(np.array(title_count) > 7) / len(title_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960\n",
      "1960\n"
     ]
    }
   ],
   "source": [
    "# build dense final prediction for evaluation\n",
    "final_pred = [json.loads(l) for l in open(\"/private/home/xwhan/code/KernelGAT/kgat/output/dense_bert_dev_mhop_top4.json\").readlines()]\n",
    "final_retrieval = [json.loads(l) for l in open(\"/private/home/xwhan/code/KernelGAT/data/bert_dense_top4_mhop_sents.json\").readlines()]\n",
    "all_dev_gold = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/shared_task_dev.jsonl\").readlines()]\n",
    "id2gold = {_[\"id\"]:_ for _ in all_dev_gold}\n",
    "\n",
    "print(len(final_retrieval))\n",
    "final = []\n",
    "for pred, retrieval in zip(final_pred, final_retrieval):\n",
    "    assert pred[\"id\"] == retrieval[\"id\"]\n",
    "    final.append({\n",
    "        \"id\": pred[\"id\"],\n",
    "        \"label\": id2gold[pred[\"id\"]][\"label\"],\n",
    "        \"evidence\": id2gold[pred[\"id\"]][\"evidence\"],\n",
    "        \"predicted_label\": pred[\"predicted_label\"],\n",
    "        \"predicted_evidence\": [[normalize(e[0]), int(e[1])] for e in retrieval[\"evidence\"][:5]]\n",
    "    })\n",
    "\n",
    "print(len(final))\n",
    "with open(\"/private/home/xwhan/data/fever/results/dense_top4_mhop_dev.json\", \"w\") as g:\n",
    "    for l in final:\n",
    "        g.write(json.dumps(l) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960\n",
      "1960\n"
     ]
    }
   ],
   "source": [
    "# build EL final prediction for evaluation\n",
    "final_pred = [json.loads(l) for l in open(\"/private/home/xwhan/code/KernelGAT/kgat/output/el_bert_dev_mhop.json\").readlines()]\n",
    "final_retrieval = [json.loads(l) for l in open(\"/private/home/xwhan/code/KernelGAT/data/bert_dev_multi_el.json\").readlines()]\n",
    "all_dev_gold = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/shared_task_dev.jsonl\").readlines()]\n",
    "id2gold = {_[\"id\"]:_ for _ in all_dev_gold}\n",
    "\n",
    "print(len(final_retrieval))\n",
    "final = []\n",
    "for pred, retrieval in zip(final_pred, final_retrieval):\n",
    "    assert pred[\"id\"] == retrieval[\"id\"]\n",
    "    final.append({\n",
    "        \"id\": pred[\"id\"],\n",
    "        \"label\": id2gold[pred[\"id\"]][\"label\"],\n",
    "        \"evidence\": id2gold[pred[\"id\"]][\"evidence\"],\n",
    "        \"predicted_label\": pred[\"predicted_label\"],\n",
    "        \"predicted_evidence\": [[normalize(e[0]), int(e[1])] for e in retrieval[\"evidence\"][:5]]\n",
    "    })\n",
    "\n",
    "print(len(final))\n",
    "with open(\"/private/home/xwhan/data/fever/results/el_mhop_dev.json\", \"w\") as g:\n",
    "    for l in final:\n",
    "        g.write(json.dumps(l) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960\n"
     ]
    }
   ],
   "source": [
    "final_pred = [json.loads(l) for l in open(\"/private/home/xwhan/code/KernelGAT/kgat/output/esim_mhop_dev.json\").readlines()]\n",
    "final_retrieval = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/retrieval/esim_mhop_dev.json\").readlines()]\n",
    "all_dev_gold = [json.loads(l) for l in open(\"/private/home/xwhan/data/fever/shared_task_dev.jsonl\").readlines()]\n",
    "id2gold = {_[\"id\"]:_ for _ in all_dev_gold}\n",
    "\n",
    "final = []\n",
    "for pred, retrieval in zip(final_pred, final_retrieval):\n",
    "    assert pred[\"id\"] == retrieval[\"id\"]\n",
    "    final.append({\n",
    "        \"id\": pred[\"id\"],\n",
    "        \"label\": id2gold[pred[\"id\"]][\"label\"],\n",
    "        \"evidence\": id2gold[pred[\"id\"]][\"evidence\"],\n",
    "        \"predicted_label\": pred[\"predicted_label\"],\n",
    "        \"predicted_evidence\": [[e[0], int(e[1])] for e in retrieval[\"evidence\"][:5]]\n",
    "    })\n",
    "\n",
    "print(len(final))\n",
    "with open(\"/private/home/xwhan/data/fever/results/esim_mhop_dev.json\", \"w\") as g:\n",
    "    for l in final:\n",
    "        g.write(json.dumps(l) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
